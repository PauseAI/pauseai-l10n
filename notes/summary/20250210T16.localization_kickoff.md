# Localization System Design - Initial Architecture

## Core Architecture

1. Fundamental Approach
   - LLM-based translation with no manual overrides
   - Cache focused on LLM requests rather than just translations
   - Build-time translation integration
   - Whole-page translation strategy
   - Git-based version control of all artifacts

2. Key Principles
   - Trust capable models (GPT-4/Claude) more, reduce preprocessing
   - Maintain full context for auditability
   - Focus on prompt engineering over code complexity
   - Preview-based workflow for validation
   - Simple validation over complex preservation rules

## Technical Implementation

1. Cache Structure
   ```
   cache/
   ├── en-US/                    
   │   └── [content-hash].[human-clues].json   # Original content
   ├── es-ES/                       
   │   └── [content-hash].[human-clues].json   # Contains translations and metadata
   └── ...                       
   ```

2. Cache Entry Format
   ```json
   {
     "sourceHash": "hash-of-original-content",
     "sourceContent": "Original en-US content",
     "translations": [{
       "content": "Translated content",
       "timestamp": "2024-03-20T12:00:00Z",
       "model": "gpt-4",
       "prompt": {
         "template": "default",
         "version": "v1.2"
       },
       "feedback": [],
       "status": "active"
     }],
     "metadata": {
       "path": "about/mission.md",
       "type": "page",
       "lastModified": "2024-03-20T12:00:00Z"
     }
   }
   ```

3. Translation Flow
   - Hash source content for cache key
   - Check cache for existing translation
   - Generate new translations if needed
   - Store full context in cache
   - Generate static pages per locale

4. Prompt Management
   ```
   prompts/
   ├── default.txt               # Base template
   └── locale-variants/          # Optional additions
       ├── es.txt
       └── de.txt
   ```

## Development Status

1. Completed
   - Initial architecture design
   - Cache structure definition
   - Basic project setup
   - Documentation framework

2. In Progress
   - Content hashing mechanism
   - Cache management implementation
   - Basic LLM integration
   - Build system integration

3. Next Steps
   - Implement core infrastructure
   - Develop prompt templates
   - Create preview system
   - Set up monitoring

## Technical Decisions

1. Repository Structure
   - Independent repository (vs monorepo)
   - TypeScript-based implementation
   - Build-time integration
   - Git-based cache storage

2. Model Strategy
   - Model selection in code
   - Runtime cost/quality decisions
   - Flexible model experimentation
   - Focus on capable models initially

3. Cache Design
   - Content-hash based
   - Human-readable hints in filenames
   - Full context preservation
   - Simple but complete metadata

4. Build Integration
   - Static site generation
   - Preview environment
   - Incremental translation
   - Cost monitoring

## Open Questions

1. Technical Challenges
   - Optimal content granularity
   - Preview system design
   - Cost tracking implementation
   - Quality metrics definition

2. Process Questions
   - Best practices for prompt versioning
   - Feedback incorporation workflow
   - Cache pruning strategy
   - Model selection criteria

3. Integration Details
   - Build system hooks
   - Preview environment setup
   - Monitoring implementation
   - Cost control mechanisms

## References
- Original discussion: `notes/raw_chat/20250210T16.localization_kickoff.{json,txt}`
- Personal notes: `notes/personal/20250210T16.localization_kickoff.md`
